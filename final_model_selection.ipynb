{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116a19d6-b7d0-49b8-8091-7ef45cf5c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: numpy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.11/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.11/site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2adf9b0-2788-437a-84c4-8d50d0dd590c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.11/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.11/site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e0b9e5-c936-46e0-ab5b-f665d680b726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the transformers library if not already available\n",
    "#!pip install transformers\n",
    "\n",
    "# Now, import the required modules\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db12043-d93b-4d8f-8772-d0043537d677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: numpy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.local/lib/python3.11/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc075e9-bce9-413c-885e-b8bd9ee9aae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update the path below to the path of your dataset in Google Drive\n",
    "#file_path = '/content/drive/My Drive/ADSproject/data/labeled_comments_cleaned.csv'\n",
    "df = pd.read_csv('labeled_comments_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b12c07-f13b-4e46-a993-2e295840e848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove symbols and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Clean the comment_full_text column\n",
    "df['comment_full_text'] = df['comment_full_text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283a57a9-2f25-4397-8416-01c7504e71ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selecting the feature and target variable\n",
    "X = df['comment_full_text']\n",
    "y = df['level_0']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f475b873-3ce4-4c5d-aaaa-5f1748d7b112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the DistilBERT model pre-trained for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4) # Adjust num_labels based on your intents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df59ffcd-a428-40b4-88ce-c9e73d08f25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c92920-6b83-4e02-a2df-285e7ab993fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert the y labels to integers\n",
    "label_dict = {v: k for k, v in enumerate(df['level_0'].unique())}\n",
    "y_train_int = y_train.replace(label_dict).values\n",
    "y_test_int = y_test.replace(label_dict).values\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = Dataset(train_encodings, y_train_int)\n",
    "test_dataset = Dataset(test_encodings, y_test_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b19b9e7-b443-466f-b2fd-1ddfcbc61cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model and logs\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    learning_rate=2e-5, # you can adjust this learning rate\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ac8353-c036-41ff-93dc-3fe6c6cc6f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.708200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.525700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.8793475131988525, metrics={'train_runtime': 73.9472, 'train_samples_per_second': 161.953, 'train_steps_per_second': 5.071, 'total_flos': 1499725188428544.0, 'train_loss': 0.8793475131988525, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8d7198-598c-49e6-be4b-af8250ae834d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.470600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.386400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.463700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.349100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Arguments for training\n",
    "training_args = TrainingArguments(\n",
    "    # ... other arguments ...\n",
    "    output_dir='./results',          # output directory for model and logs\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    learning_rate=2e-5, # you can adjust this learning rate\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
    "    # fp16=True,  # Enable mixed precision training\n",
    "    save_steps=200,  # Save model every 200 steps\n",
    "    save_total_limit=2,  # Only keep the last two checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Training in a loop to save intermittently\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    # trainer.train(resume_from_checkpoint=True)\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"./model_checkpoint_epoch_{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fad854-a741-4aa0-9884-c42e9810eb89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6235858798027039,\n",
       " 'eval_runtime': 2.1059,\n",
       " 'eval_samples_per_second': 474.374,\n",
       " 'eval_steps_per_second': 15.195,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88aef54b-38ee-49c6-a033-17d38f2fa782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error Rate: 0.224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming predictions are made using the Trainer's predict method on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get the predicted labels by taking the argmax of the logits from the predictions\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Calculate accuracy using the true labels and the predicted labels\n",
    "accuracy = accuracy_score(predictions.label_ids, predicted_labels)\n",
    "\n",
    "# The test error rate is 1 minus the accuracy\n",
    "test_error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Test Error Rate: {test_error_rate:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5affb1-ba8f-4191-a8ce-66ca552316bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Can we include the latest survey data in our analysis?'\n",
      "Predicted intent: LABEL_1\n",
      "\n",
      "Text: 'Great teamwork on the recent launch, everyone should be proud!'\n",
      "Predicted intent: LABEL_1\n",
      "\n",
      "Text: 'Where can I find the financial report for Q2?'\n",
      "Predicted intent: LABEL_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Make sure to load the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # Replace with your model name if different\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Example sentences for prediction\n",
    "sentences = [\n",
    "    \"Can we include the latest survey data in our analysis?\",\n",
    "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
    "    \"Where can I find the financial report for Q2?\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the tokenized inputs to the device\n",
    "input_ids = encoded_inputs[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the predictions\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Move predictions back to CPU for further operations if needed\n",
    "predictions = predictions.detach().cpu().numpy()\n",
    "\n",
    "# Convert the predictions to label names if you have a mapping\n",
    "# If using a model from Hugging Face with an associated config, you can convert the indices to labels directly\n",
    "label_names = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "# Output the predictions\n",
    "for sentence, label in zip(sentences, label_names):\n",
    "    print(f\"Text: '{sentence}'\\nPredicted intent: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f116f445-19a9-49d5-a3bf-b3a666d5cc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"Can we include the latest survey data in our analysis?\"\n",
      "Predicted intent: Information Exchange\n",
      "\n",
      "Text: \"Great teamwork on the recent launch, everyone should be proud!\"\n",
      "Predicted intent: Social Communication\n",
      "\n",
      "Text: \"Where can I find the financial report for Q2?\"\n",
      "Predicted intent: Information Exchange\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4) # Replace NUM_LABELS with the actual number of labels in your task\n",
    "\n",
    "# Tokenize the input texts\n",
    "texts = [\n",
    "    \"Can we include the latest survey data in our analysis?\",\n",
    "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
    "    \"Where can I find the financial report for Q2?\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the model and inputs to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model and logs\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # replace with your actual training dataset\n",
    "    eval_dataset=test_dataset,    # replace with your actual evaluation dataset\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()  # Put the model in evaluation mode\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert logits to class labels\n",
    "predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Map the predictions to actual labels (you need to replace LABEL_MAPPING with your actual label mapping)\n",
    "LABEL_MAPPING = {0: \"Information Exchange\", 1: \"Modification\", 2: \"Social Communication\", 3: \"Other\"}  # Replace with actual mapping\n",
    "predicted_labels = [LABEL_MAPPING[label] for label in predictions]\n",
    "\n",
    "# Print the predictions\n",
    "for text, label in zip(texts, predicted_labels):\n",
    "    print(f'Text: \"{text}\"\\nPredicted intent: {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09130415-a1fc-482f-9d1d-77e8167be81d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error Rate: 0.233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming predictions are made using the Trainer's predict method on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get the predicted labels by taking the argmax of the logits from the predictions\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Calculate accuracy using the true labels and the predicted labels\n",
    "accuracy = accuracy_score(predictions.label_ids, predicted_labels)\n",
    "\n",
    "# The test error rate is 1 minus the accuracy\n",
    "test_error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Test Error Rate: {test_error_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43329e97-1196-4dfb-821e-b61f767ad00f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"Is there an update on the project timeline?\"\n",
      "Predicted intent: Modification\n",
      "\n",
      "Text: \"Congratulations on reaching your sales targets!\"\n",
      "Predicted intent: Modification\n",
      "\n",
      "Text: \"Could you provide me with the meeting minutes?\"\n",
      "Predicted intent: Modification\n",
      "\n",
      "Text: \"Let's schedule a review meeting for Monday.\"\n",
      "Predicted intent: Modification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Texts to predict\n",
    "texts_to_predict = [\n",
    "    \"Is there an update on the project timeline?\",\n",
    "    \"Congratulations on reaching your sales targets!\",\n",
    "    \"Could you provide me with the meeting minutes?\",\n",
    "    \"Let's schedule a review meeting for Monday.\"\n",
    "]\n",
    "\n",
    "# Tokenize the texts\n",
    "inputs = tokenizer(texts_to_predict, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the inputs to the same device as the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)  # Replace num_labels with your actual number\n",
    "model.to(device)\n",
    "\n",
    "# Get the model's predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Map predictions to the actual labels\n",
    "# Replace this with your actual label mapping\n",
    "label_mapping = {0: 'Information Exchange', 1: 'Modification', 2: 'Social Communication', 3: 'Other'}\n",
    "predicted_labels = [label_mapping[pred.item()] for pred in predictions]\n",
    "\n",
    "# Display predictions\n",
    "for text, label in zip(texts_to_predict, predicted_labels):\n",
    "    print(f'Text: \"{text}\"\\nPredicted intent: {label}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d330cd6-ba88-45d0-a88a-28f6f5496451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/spati/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='423' max='423' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [423/423 01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762555</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.618034</td>\n",
       "      <td>0.726655</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.641544</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750608</td>\n",
       "      <td>0.759001</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559903</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.772548</td>\n",
       "      <td>0.792075</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.txt',\n",
       " './trained_model/added_tokens.json',\n",
       " './trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the required packages if running in a Google Colab environment\n",
    "# !pip install transformers datasets sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define a function to compute metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_comments_cleaned.csv')  # Replace with your file path\n",
    "\n",
    "# Preprocess your data if necessary (e.g., you might want to clean text data)\n",
    "\n",
    "# Map labels to integers\n",
    "label_list = data['level_0'].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "train_data['labels'] = train_data['level_0'].map(label_to_id)\n",
    "val_data['labels'] = val_data['level_0'].map(label_to_id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['comment_full_text'], padding=True, truncation=True)\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
    "\n",
    "# Convert datasets to the Torch format\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./trained_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6adf3c8-91b3-412b-9987-f69d0add9deb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFORMATION EXCHANGE', 'SOCIAL COMMUNICATION', 'INFORMATION EXCHANGE']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model_path = './trained_model'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(texts):\n",
    "    # Tokenize the texts\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Move tensor to the right device\n",
    "    encoded_input.to(model.device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "\n",
    "    # Get the predicted class indices\n",
    "    predictions = torch.argmax(probabilities, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Convert predicted class indices to labels\n",
    "    predicted_labels = [id_to_label[idx] for idx in predictions]\n",
    "    return predicted_labels\n",
    "\n",
    "# Example usage:\n",
    "texts = [\n",
    "    \"Can we include the latest survey data in our analysis?\",\n",
    "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
    "    \"Where can I find the financial report for Q2?\"\n",
    "]\n",
    "\n",
    "predicted_labels = predict(texts)\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2150f72-b79f-4983-8f86-e2bdcd06aba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Is there an update on the project timeline?'\n",
      "Predicted intent: Information Exchange\n",
      "Text: 'Congratulations on reaching your sales targets!'\n",
      "Predicted intent: Social Communication\n",
      "Text: 'Could you provide me with the meeting minutes?'\n",
      "Predicted intent: Information Exchange\n",
      "Text: 'Let's schedule a review meeting for Monday.'\n",
      "Predicted intent: Social Communication\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Make sure to replace 'path_to_your_model' with the actual path where your model is saved\n",
    "model_path = './trained_model'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Put your model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define your new examples here\n",
    "examples = [\n",
    "    \"Is there an update on the project timeline?\",\n",
    "    \"Congratulations on reaching your sales targets!\",\n",
    "    \"Could you provide me with the meeting minutes?\",\n",
    "    \"Let's schedule a review meeting for Monday.\"\n",
    "]\n",
    "\n",
    "# Tokenize and predict the intents of the new examples\n",
    "for example in examples:\n",
    "    inputs = tokenizer(example, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "    # Convert the predicted label index to the actual label name, replace this with your actual label names\n",
    "    label_names = ['Information Exchange', 'Modification', 'Social Communication', 'Other']\n",
    "    predicted_intent = label_names[predicted_label]\n",
    "\n",
    "    print(f\"Text: '{example}'\")\n",
    "    print(f\"Predicted intent: {predicted_intent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f855fb-4c1b-4a08-9d13-dcb6398707ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.3378965854644775, 'test_accuracy': 0.37327188940092165, 'test_f1': 0.35120621888810905, 'test_precision': 0.33668536140838046, 'test_recall': 0.37327188940092165, 'test_runtime': 15.7201, 'test_samples_per_second': 317.492, 'test_steps_per_second': 19.847}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.txt',\n",
       " './trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "test_dataframe = pd.read_csv('labeled_comments_cleaned.csv')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the test dataset\n",
    "test_encodings = tokenizer(test_dataframe['comment_full_text'].tolist(), truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Correct the label_to_id dictionary to match the case used in your dataframe\n",
    "label_to_id = {\n",
    "    'Information Exchange': 0,\n",
    "    'Modification': 1,\n",
    "    'Social Communication': 2,\n",
    "    'Other': 3\n",
    "}\n",
    "\n",
    "# Ensure your dataframe 'level_0' column has labels that match the keys in label_to_id\n",
    "# If not, you need to convert them to match. Here's how you can do it:\n",
    "test_dataframe['level_0'] = test_dataframe['level_0'].str.title()  # Convert to title case to match keys\n",
    "\n",
    "# Convert the list of labels to the correct format, mapping each label to its corresponding ID\n",
    "true_labels = [label_to_id[label] for label in test_dataframe['level_0']]\n",
    "\n",
    "# Create a test dataset\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': true_labels})\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_to_id))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    do_predict=True\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert the predictions to labels\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Calculate metrics\n",
    "def compute_metrics(p):\n",
    "    pred_labels = np.argmax(p.predictions, axis=-1)\n",
    "    accuracy = accuracy_score(p.label_ids, pred_labels)\n",
    "    f1 = f1_score(p.label_ids, pred_labels, average='weighted')\n",
    "    precision = precision_score(p.label_ids, pred_labels, average='weighted')\n",
    "    recall = recall_score(p.label_ids, pred_labels, average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Assuming train_dataset is available\n",
    "    eval_dataset=val_dataset,  # Assuming val_dataset is available\n",
    "    compute_metrics=compute_metrics  # Pass the compute_metrics function here\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "print(eval_results)\n",
    "\n",
    "# Remember to save the trained model and tokenizer if needed\n",
    "model_path = \"./trained_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a9b13b-a8cb-459e-8c54-e765e3d5a3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37\n",
      "Precision: 0.34\n",
      "Recall: 0.37\n",
      "F1 Score: 0.35\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `true_labels` and `pred_labels` are your actual and predicted labels respectively\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision = precision_score(true_labels, pred_labels, average='weighted')  # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
    "recall = recall_score(true_labels, pred_labels, average='weighted')        # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
    "f1 = f1_score(true_labels, pred_labels, average='weighted')                # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Generate and visualize the confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff454d6f-ba92-4810-9fed-307eba2893c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/spati/.local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='423' max='423' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [423/423 01:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.775278</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.619165</td>\n",
       "      <td>0.588503</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.664673</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.742325</td>\n",
       "      <td>0.755988</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.577033</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.752658</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.752000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.txt',\n",
       " './trained_model/added_tokens.json',\n",
       " './trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define a function to compute metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_comments_cleaned.csv')  # Replace with your file path\n",
    "\n",
    "# Preprocess your data if necessary (e.g., you might want to clean text data)\n",
    "\n",
    "# Map labels to integers\n",
    "label_list = data['level_0'].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "train_data['labels'] = train_data['level_0'].map(label_to_id)\n",
    "val_data['labels'] = val_data['level_0'].map(label_to_id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['comment_full_text'], padding=True, truncation=True)\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
    "\n",
    "# Convert datasets to the Torch format\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./trained_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
